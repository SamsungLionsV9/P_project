"""
ì¼ë°˜ êµ­ì‚°ì°¨ ì „ìš© ëª¨ë¸ í•™ìŠµ (ì œë„¤ì‹œìŠ¤ ì œì™¸)
- í˜„ëŒ€, ê¸°ì•„, ì‰ë³´ë ˆ, KGëª¨ë¹Œë¦¬í‹°, ë¥´ë…¸ì½”ë¦¬ì•„ ë“±
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import joblib
import os

def create_regular_features(df):
    """ì¼ë°˜ êµ­ì‚°ì°¨ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
    print("  ğŸ”§ ì¼ë°˜ êµ­ì‚°ì°¨ í”¼ì²˜ ìƒì„± ì¤‘...")
    
    current_year = 2025
    df['age'] = current_year - df['year']
    
    df['mileage_per_year'] = df['mileage'] / (df['age'] + 1)
    df['is_low_mileage'] = (df['mileage'] < 30000).astype(int)
    df['is_high_mileage'] = (df['mileage'] > 150000).astype(int)
    
    df['age_group'] = pd.cut(df['age'], 
                              bins=[-1, 1, 3, 5, 10, 100], 
                              labels=['new', 'semi_new', 'used', 'old', 'very_old'])
    
    df['brand_fuel'] = df['brand'] + '_' + df['fuel']
    
    model_counts = df['model_name'].value_counts()
    df['model_popularity'] = df['model_name'].map(model_counts)
    df['model_popularity_log'] = np.log1p(df['model_popularity'])
    
    df['age_mileage_interaction'] = df['age'] * np.log1p(df['mileage'])
    
    brand_price_mean = df.groupby('brand')['price'].transform('mean')
    df['brand_price_tier'] = pd.cut(brand_price_mean, bins=3, labels=['budget', 'mid', 'premium'])
    
    df['is_eco'] = df['fuel'].str.contains('ì „ê¸°|í•˜ì´ë¸Œë¦¬ë“œ', na=False).astype(int)
    
    df['log_price'] = np.log1p(df['price'])
    
    print(f"     âœ“ ìƒì„±ëœ í”¼ì²˜ ìˆ˜: {len([c for c in df.columns if c not in ['price', 'log_price']])}")
    
    return df

def train_regular_domestic_model(data_path='../data/processed_encar_combined.csv',
                                 model_path='../models/regular_domestic_model.pkl'):
    """ì¼ë°˜ êµ­ì‚°ì°¨ ëª¨ë¸ í•™ìŠµ"""
    
    print("\n" + "="*70)
    print("  ğŸš— ì¼ë°˜ êµ­ì‚°ì°¨ ëª¨ë¸ í•™ìŠµ (ì œë„¤ì‹œìŠ¤ ì œì™¸)")
    print("="*70)
    
    # ë°ì´í„° ë¡œë“œ
    print("\n  ğŸ“‚ ë°ì´í„° ë¡œë”©...")
    df = pd.read_csv(data_path)
    
    # êµ­ì‚°ì°¨ ì¤‘ ì œë„¤ì‹œìŠ¤ ì œì™¸
    df = df[(df['car_type'] == 'Domestic') & (df['brand'] != 'ì œë„¤ì‹œìŠ¤')].copy()
    print(f"     ì¼ë°˜ êµ­ì‚°ì°¨ ë°ì´í„°: {len(df):,}ê±´")
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    df = create_regular_features(df)
    
    # ì´ìƒì¹˜ ì œê±°
    initial_count = len(df)
    df = df[(df['price'] >= 100) & (df['price'] <= 8000)]  # 100ë§Œ~8000ë§Œì›
    removed = initial_count - len(df)
    print(f"  ğŸ§¹ ì´ìƒì¹˜ ì œê±°: {removed:,}ê±´ (100ë§Œì› ë¯¸ë§Œ ë˜ëŠ” 8000ë§Œì› ì´ˆê³¼)")
    print(f"  ìµœì¢… ë°ì´í„°: {len(df):,}ê±´")
    print(f"  ê°€ê²© ë²”ìœ„: {df['price'].min():.0f}ë§Œì› ~ {df['price'].max():.0f}ë§Œì›")
    
    # í”¼ì²˜ ì„ íƒ
    categorical_features = ['brand', 'model_name', 'fuel', 'age_group', 'brand_fuel', 'brand_price_tier']
    numerical_features = [
        'age', 'mileage', 'mileage_per_year',
        'is_low_mileage', 'is_high_mileage',
        'model_popularity_log', 'age_mileage_interaction', 'is_eco'
    ]
    
    categorical_features = [f for f in categorical_features if f in df.columns]
    numerical_features = [f for f in numerical_features if f in df.columns]
    
    feature_cols = categorical_features + numerical_features
    X = df[feature_cols]
    y = df['log_price']
    
    # Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"\n  ğŸ“Š Train: {len(X_train):,} | Test: {len(X_test):,}")
    
    # Preprocessing
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
            ('num', 'passthrough', numerical_features)
        ]
    )
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    xgb_params = {
        'n_estimators': 1000,
        'learning_rate': 0.05,
        'max_depth': 7,
        'min_child_weight': 3,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'objective': 'reg:squarederror',
        'random_state': 42,
        'n_jobs': -1
    }
    
    # Pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', XGBRegressor(**xgb_params))
    ])
    
    # í•™ìŠµ
    print(f"\n  ğŸš€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    pipeline.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)
    
    y_train_actual = np.expm1(y_train)
    y_test_actual = np.expm1(y_test)
    y_train_pred_actual = np.expm1(y_train_pred)
    y_test_pred_actual = np.expm1(y_test_pred)
    
    # í‰ê°€
    train_mae = mean_absolute_error(y_train_actual, y_train_pred_actual)
    test_mae = mean_absolute_error(y_test_actual, y_test_pred_actual)
    train_r2 = r2_score(y_train_actual, y_train_pred_actual)
    test_r2 = r2_score(y_test_actual, y_test_pred_actual)
    
    train_mape = np.mean(np.abs((y_train_actual - y_train_pred_actual) / y_train_actual)) * 100
    test_mape = np.mean(np.abs((y_test_actual - y_test_pred_actual) / y_test_actual)) * 100
    
    print(f"\n  ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"     Train MAE: {train_mae:.0f}ë§Œì› | MAPE: {train_mape:.2f}%")
    print(f"     Test MAE:  {test_mae:.0f}ë§Œì› | MAPE: {test_mape:.2f}%")
    print(f"     Train RÂ²:  {train_r2:.4f}")
    print(f"     Test RÂ²:   {test_r2:.4f}")
    
    # Cross Validation
    print(f"\n  ğŸ”„ K-Fold Cross Validation (k=5)...")
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(pipeline, X, y, cv=kfold, scoring='r2', n_jobs=-1)
    print(f"     CV RÂ² scores: {cv_scores}")
    print(f"     í‰ê·  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    r2_gap = abs(train_r2 - cv_scores.mean())
    if r2_gap < 0.05:
        print(f"     âœ… ê³¼ì í•© ì—†ìŒ (Train-CV ì°¨ì´: {r2_gap:.4f})")
    else:
        print(f"     âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„± (Train-CV ì°¨ì´: {r2_gap:.4f})")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(pipeline, model_path)
    print(f"\n  âœ… ëª¨ë¸ ì €ì¥: {model_path}")
    
    return {
        'data_count': len(df),
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'cv_r2': cv_scores.mean(),
        'test_mape': test_mape
    }

if __name__ == "__main__":
    result = train_regular_domestic_model()
    
    print("\n" + "="*70)
    print("  ğŸ‰ ì¼ë°˜ êµ­ì‚°ì°¨ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("="*70)
