"""
ì œë„¤ì‹œìŠ¤ ì „ìš© ëª¨ë¸ í•™ìŠµ
- í”„ë¦¬ë¯¸ì—„ êµ­ì‚°ì°¨ ë¸Œëœë“œ íŠ¹í™”
- ë¸Œëœë“œ í”„ë¦¬ë¯¸ì—„ ë°˜ì˜
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import joblib
import os

def create_genesis_features(df):
    """ì œë„¤ì‹œìŠ¤ íŠ¹í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
    print("  ğŸ”§ ì œë„¤ì‹œìŠ¤ ì „ìš© í”¼ì²˜ ìƒì„± ì¤‘...")
    
    current_year = 2025
    df['age'] = current_year - df['year']
    
    # ê¸°ë³¸ í”¼ì²˜
    df['mileage_per_year'] = df['mileage'] / (df['age'] + 1)
    df['is_low_mileage'] = (df['mileage'] < 30000).astype(int)
    df['is_high_mileage'] = (df['mileage'] > 100000).astype(int)
    
    # ì—°ë ¹ ê·¸ë£¹
    df['age_group'] = pd.cut(df['age'], 
                              bins=[-1, 1, 3, 5, 10, 100], 
                              labels=['new', 'semi_new', 'used', 'old', 'very_old'])
    
    # ëª¨ë¸ ì¸ê¸°ë„
    model_counts = df['model_name'].value_counts()
    df['model_popularity'] = df['model_name'].map(model_counts)
    df['model_popularity_log'] = np.log1p(df['model_popularity'])
    
    # ì œë„¤ì‹œìŠ¤ íŠ¹í™” í”¼ì²˜
    
    # 1. ëª¨ë¸ í‹°ì–´ (G70 < G80 < G90)
    df['model_tier'] = 'mid'
    df.loc[df['model_name'].str.contains('G70|GV70', na=False), 'model_tier'] = 'entry'
    df.loc[df['model_name'].str.contains('G90|GV90', na=False), 'model_tier'] = 'luxury'
    
    # 2. SUV vs ì„¸ë‹¨
    df['is_suv'] = df['model_name'].str.contains('GV', na=False).astype(int)
    
    # 3. ê°€ê²©ëŒ€ë³„ êµ¬ë¶„
    # ì œë„¤ì‹œìŠ¤ëŠ” ì‹ ì°¨ ê°€ê²©ì´ ëª…í™•í•¨
    model_price_map = {
        'G70': 4500,
        'G80': 5500,
        'G90': 9000,
        'GV70': 5000,
        'GV80': 6500,
        'GV90': 10000
    }
    
    def get_base_price(model_name):
        for key, price in model_price_map.items():
            if key in str(model_name):
                return price
        return 5000  # default
    
    df['model_base_price'] = df['model_name'].apply(get_base_price)
    
    # 4. ê°ê°€ìƒê°ë¥  (ë‚˜ì´ë³„)
    # ì œë„¤ì‹œìŠ¤ëŠ” ì¼ë°˜ êµ­ì‚°ì°¨ë³´ë‹¤ ê°€ì¹˜ ìœ ì§€ ì˜ë¨
    df['depreciation_rate'] = 1 - (df['age'] * 0.12)  # ë…„ë‹¹ 12% ê°ê°€
    df['depreciation_rate'] = df['depreciation_rate'].clip(0.3, 1.0)
    
    # 5. í¬ì†Œì„± (ì ì„ìˆ˜ë¡ í¬ì†Œ)
    df['rarity_score'] = 1 / (df['model_popularity'] + 1)
    
    # 6. ëŸ­ì…”ë¦¬ ì˜µì…˜ ì¶”ì • (ê°€ê²©ì´ ëª¨ë¸ í‰ê· ë³´ë‹¤ ë†’ìœ¼ë©´ í’€ì˜µ)
    model_price_mean = df.groupby('model_name')['price'].transform('mean')
    df['is_high_trim'] = (df['price'] > model_price_mean * 1.1).astype(int)
    
    # 7. ì—°ë£Œ íš¨ìœ¨ (í•˜ì´ë¸Œë¦¬ë“œ/ì „ê¸°)
    df['is_eco'] = df['fuel'].str.contains('ì „ê¸°|í•˜ì´ë¸Œë¦¬ë“œ', na=False).astype(int)
    
    # 8. ìƒíƒœ ì§€í‘œ (ì£¼í–‰ê±°ë¦¬ vs ì—°ì‹)
    df['condition_score'] = df['mileage_per_year'] / 15000  # ë…„ 15,000km ê¸°ì¤€
    df['condition_score'] = df['condition_score'].clip(0, 3)
    
    # ë¡œê·¸ ë³€í™˜ (íƒ€ê²Ÿ)
    df['log_price'] = np.log1p(df['price'])
    
    print(f"     âœ“ ìƒì„±ëœ í”¼ì²˜ ìˆ˜: {len([c for c in df.columns if c not in ['price', 'log_price']])}")
    
    return df

def train_genesis_model(data_path='../data/processed_encar_combined.csv',
                       model_path='../models/genesis_car_price_model.pkl'):
    """ì œë„¤ì‹œìŠ¤ ëª¨ë¸ í•™ìŠµ"""
    
    print("\n" + "="*70)
    print("  ğŸ† ì œë„¤ì‹œìŠ¤ ì „ìš© ëª¨ë¸ í•™ìŠµ")
    print("="*70)
    
    # ë°ì´í„° ë¡œë“œ
    print("\n  ğŸ“‚ ë°ì´í„° ë¡œë”©...")
    df = pd.read_csv(data_path)
    
    # ì œë„¤ì‹œìŠ¤ë§Œ í•„í„°ë§
    df = df[df['brand'] == 'ì œë„¤ì‹œìŠ¤'].copy()
    print(f"     ì œë„¤ì‹œìŠ¤ ë°ì´í„°: {len(df):,}ê±´")
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    df = create_genesis_features(df)
    
    # ì´ìƒì¹˜ ì œê±° (ê·¹ë‹¨ì ì¸ ê²½ìš°ë§Œ)
    initial_count = len(df)
    df = df[(df['price'] >= 500) & (df['price'] <= 20000)]  # 500ë§Œ~2ì–µ
    removed = initial_count - len(df)
    print(f"  ğŸ§¹ ê·¹ë‹¨ ì´ìƒì¹˜ ì œê±°: {removed:,}ê±´")
    print(f"  ìµœì¢… ë°ì´í„°: {len(df):,}ê±´")
    print(f"  ê°€ê²© ë²”ìœ„: {df['price'].min():.0f}ë§Œì› ~ {df['price'].max():.0f}ë§Œì›")
    
    # í”¼ì²˜ ì„ íƒ
    categorical_features = ['model_name', 'fuel', 'age_group', 'model_tier']
    numerical_features = [
        'age', 'mileage', 'mileage_per_year',
        'is_low_mileage', 'is_high_mileage',
        'model_popularity_log', 'is_suv',
        'model_base_price', 'depreciation_rate',
        'rarity_score', 'is_high_trim',
        'is_eco', 'condition_score'
    ]
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ
    categorical_features = [f for f in categorical_features if f in df.columns]
    numerical_features = [f for f in numerical_features if f in df.columns]
    
    feature_cols = categorical_features + numerical_features
    X = df[feature_cols]
    y = df['log_price']
    
    # Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"\n  ğŸ“Š Train: {len(X_train):,} | Test: {len(X_test):,}")
    
    # Preprocessing
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
            ('num', 'passthrough', numerical_features)
        ]
    )
    
    # ì œë„¤ì‹œìŠ¤ ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°
    xgb_params = {
        'n_estimators': 1200,      # ì¤‘ê°„ ë°ì´í„° í¬ê¸°ì— ì í•©
        'learning_rate': 0.04,     # ì²œì²œíˆ í•™ìŠµ
        'max_depth': 6,            # ê³¼ì í•© ë°©ì§€
        'min_child_weight': 2,
        'subsample': 0.85,
        'colsample_bytree': 0.85,
        'gamma': 0.1,              # ê³¼ì í•© ë°©ì§€
        'reg_alpha': 0.1,          # L1 ê·œì œ
        'reg_lambda': 1.0,         # L2 ê·œì œ
        'objective': 'reg:squarederror',
        'random_state': 42,
        'n_jobs': -1
    }
    
    # Pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', XGBRegressor(**xgb_params))
    ])
    
    # í•™ìŠµ
    print(f"\n  ğŸš€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    pipeline.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ
    y_train_actual = np.expm1(y_train)
    y_test_actual = np.expm1(y_test)
    y_train_pred_actual = np.expm1(y_train_pred)
    y_test_pred_actual = np.expm1(y_test_pred)
    
    # í‰ê°€
    train_mae = mean_absolute_error(y_train_actual, y_train_pred_actual)
    test_mae = mean_absolute_error(y_test_actual, y_test_pred_actual)
    train_r2 = r2_score(y_train_actual, y_train_pred_actual)
    test_r2 = r2_score(y_test_actual, y_test_pred_actual)
    
    train_mape = np.mean(np.abs((y_train_actual - y_train_pred_actual) / y_train_actual)) * 100
    test_mape = np.mean(np.abs((y_test_actual - y_test_pred_actual) / y_test_actual)) * 100
    
    print(f"\n  ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"     Train MAE: {train_mae:.0f}ë§Œì› | MAPE: {train_mape:.2f}%")
    print(f"     Test MAE:  {test_mae:.0f}ë§Œì› | MAPE: {test_mape:.2f}%")
    print(f"     Train RÂ²:  {train_r2:.4f}")
    print(f"     Test RÂ²:   {test_r2:.4f}")
    
    # Cross Validation
    print(f"\n  ğŸ”„ K-Fold Cross Validation (k=5)...")
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(pipeline, X, y, cv=kfold, scoring='r2', n_jobs=-1)
    print(f"     CV RÂ² scores: {cv_scores}")
    print(f"     í‰ê·  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    # ê³¼ì í•© ê²€ì‚¬
    r2_gap = abs(train_r2 - cv_scores.mean())
    if r2_gap < 0.05:
        print(f"     âœ… ê³¼ì í•© ì—†ìŒ (Train-CV ì°¨ì´: {r2_gap:.4f})")
    else:
        print(f"     âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„± (Train-CV ì°¨ì´: {r2_gap:.4f})")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(pipeline, model_path)
    print(f"\n  âœ… ëª¨ë¸ ì €ì¥: {model_path}")
    
    # ìƒ˜í”Œ ì˜ˆì¸¡
    print(f"\n  ğŸ“‹ ì˜ˆì¸¡ ìƒ˜í”Œ (Random 10ê±´):")
    sample_idx = np.random.choice(len(X_test), min(10, len(X_test)), replace=False)
    
    for i, idx in enumerate(sample_idx):
        actual = y_test_actual.iloc[idx]
        pred = y_test_pred_actual[idx]
        error = abs(actual - pred)
        error_pct = error / actual * 100
        
        model = df.iloc[X_test.index[idx]]['model_name']
        year = df.iloc[X_test.index[idx]]['year']
        mileage = df.iloc[X_test.index[idx]]['mileage']
        
        print(f"     {model:20s} ({year:.0f}ë…„, {mileage:6.0f}km): "
              f"ì‹¤ì œ {actual:5.0f}ë§Œì› | ì˜ˆì¸¡ {pred:5.0f}ë§Œì› | "
              f"ì˜¤ì°¨ {error:4.0f}ë§Œì› ({error_pct:4.1f}%)")
    
    return {
        'data_count': len(df),
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'cv_r2': cv_scores.mean(),
        'test_mape': test_mape
    }

if __name__ == "__main__":
    result = train_genesis_model()
    
    print("\n" + "="*70)
    print("  ğŸ‰ ì œë„¤ì‹œìŠ¤ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("="*70)
