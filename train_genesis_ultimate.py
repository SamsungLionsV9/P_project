"""
ì œë„¤ì‹œìŠ¤ ì „ìš© ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ - ìµœì¢… ì†”ë£¨ì…˜
Target Encoding + ë¡œê·¸ ë³€í™˜ + ê°•ë ¥í•œ ì •ê·œí™”
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import xgboost as xgb
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸ† ì œë„¤ì‹œìŠ¤ ì „ìš© ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ - ìµœì¢… ì†”ë£¨ì…˜")
print("="*80)
print()

# ========== 1. ë°ì´í„° ë¡œë“œ ==========
print("ğŸ“‚ Step 1: ë°ì´í„° ë¡œë“œ...")

df_raw = pd.read_csv('encar_raw_domestic.csv')
df_detail = pd.read_csv('data/complete_domestic_details.csv')
df = df_raw.merge(df_detail, left_on='Id', right_on='car_id', how='inner')

# ì œë„¤ì‹œìŠ¤ë§Œ í•„í„°ë§
genesis_keywords = ['ì œë„¤ì‹œìŠ¤', 'GENESIS', 'Genesis']
df = df[df['Manufacturer'].str.contains('|'.join(genesis_keywords), case=False, na=False)]
print(f"âœ“ ì œë„¤ì‹œìŠ¤ ë°ì´í„°: {len(df):,}í–‰")

# ========== 2. ì „ì²˜ë¦¬ ==========
print("\nğŸ”§ Step 2: ì „ì²˜ë¦¬...")

df = df.dropna(subset=['Price', 'Mileage', 'Year', 'Model'])
df = df[df['Price'] > 500]
df = df[df['Price'] < 20000]
df = df[df['Mileage'] < 300000]
df = df[df['Year'] >= 2015]

# ë¡œê·¸ ë³€í™˜
df['Price_log'] = np.log1p(df['Price'])
print(f"âœ“ ì „ì²˜ë¦¬ í›„: {len(df):,}í–‰")

# ========== 3. Target Encoding ==========
print("\nâ­ Step 3: Target Encoding...")

# Train/Test ë¶„ë¦¬ (ì¸ë±ìŠ¤ ìœ ì§€!)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# ë¶„ë¦¬ ë§ˆì»¤ ì¶”ê°€
train_df['is_train'] = 1
test_df['is_train'] = 0

# Target Encoding
def create_target_encoding(train, test, col, target='Price_log', min_samples=10):
    global_mean = train[target].mean()
    agg = train.groupby(col)[target].agg(['mean', 'count'])
    counts = agg['count']
    means = agg['mean']
    
    smooth = 1 / (1 + np.exp(-(counts - min_samples) / 5))
    encoded = global_mean * (1 - smooth) + means * smooth
    
    train[f'{col}_target_enc'] = train[col].map(encoded).fillna(global_mean)
    test[f'{col}_target_enc'] = test[col].map(encoded).fillna(global_mean)
    
    return train, test, encoded

# Model Target Encoding
train_df, test_df, model_encoding = create_target_encoding(
    train_df, test_df, 'Model', 'Price_log', min_samples=20
)
print(f"âœ“ Model Target Encoding")

# ë°ì´í„° í•©ì¹˜ê¸° (is_train ë§ˆì»¤ë¡œ ë‚˜ì¤‘ì— ë¶„ë¦¬)
df = pd.concat([train_df, test_df], ignore_index=True)

# ========== 4. Feature Engineering ==========
print("\nâš™ï¸ Step 4: Feature Engineering...")

current_year = 2025
df['age'] = current_year - df['Year']
df['age_squared'] = df['age'] ** 2
df['age_cubed'] = df['age'] ** 3

# Mileage
df['mileage_per_year'] = df['Mileage'] / (df['age'] + 1)
df['mileage_log'] = np.log1p(df['Mileage'])
df['mileage_squared'] = df['Mileage'] ** 2

# ì˜µì…˜
option_cols = [
    'has_sunroof', 'has_navigation', 'has_leather_seat', 'has_smart_key', 
    'has_rear_camera', 'has_led_lamp', 'has_parking_sensor', 'has_auto_ac',
    'has_heated_seat', 'has_ventilated_seat'
]

for col in option_cols:
    df[col] = df[col].fillna(0)

df['option_score'] = df[option_cols].sum(axis=1)
df['option_rate'] = df['option_score'] / 10

# ì œë„¤ì‹œìŠ¤ëŠ” ê³ ê¸‰ ì˜µì…˜ ê°€ì¤‘ì¹˜ ë” ë†’ê²Œ
premium_weights = {
    'has_sunroof': 2.0,
    'has_ventilated_seat': 2.0,
    'has_led_lamp': 1.5,
    'has_leather_seat': 1.5,
    'has_navigation': 1.3,
    'has_smart_key': 1.2,
    'has_rear_camera': 1.0,
    'has_parking_sensor': 1.0,
    'has_auto_ac': 1.0,
    'has_heated_seat': 1.0
}

df['option_weighted'] = sum(df[col] * weight for col, weight in premium_weights.items())

# ì„±ëŠ¥ ë“±ê¸‰
grade_map = {'excellent': 3, 'good': 2, 'normal': 1}
df['inspection_score'] = df['inspection_grade'].map(grade_map).fillna(1)

# ì™„ë²½í•œ ì¡°ê±´
df['is_premium_condition'] = (
    (df['is_accident_free'] == 1) & 
    (df['inspection_score'] == 3) &
    (df['mileage_per_year'] < 10000)
).astype(int)

df['is_full_option'] = (df['option_score'] >= 9).astype(int)

# ì§€ì—­
df['region'] = df['region'].fillna('Unknown')
df['is_metro'] = (
    (df['region'].str.contains('ì„œìš¸')) | 
    (df['region'].str.contains('ê²½ê¸°'))
).astype(int)

# ìƒí˜¸ì‘ìš©
df['age_option_interaction'] = df['age'] * df['option_rate']
df['model_option_interaction'] = df['Model_target_enc'] * df['option_weighted']
df['age_mileage_interaction'] = df['age'] * df['mileage_log']

# ê°€ê²© êµ¬ê°„
price_bins = pd.qcut(df['Price'], q=5, labels=False, duplicates='drop')
df['price_segment'] = price_bins

print(f"âœ“ Feature Engineering ì™„ë£Œ")

# ========== 5. Label Encoding ==========
print("\nğŸ·ï¸ Step 5: ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©...")

encoders = {}
for col in ['FuelType']:
    if col in df.columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
        encoders[col] = le

encoders['Model_target_enc'] = model_encoding

# ========== 6. í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ==========
print("\nğŸ“Š Step 6: í•™ìŠµ ë°ì´í„° ì¤€ë¹„...")

feature_cols = [
    # ê¸°ë³¸
    'Year', 'age', 'age_squared', 'age_cubed',
    
    # ì£¼í–‰ê±°ë¦¬
    'Mileage', 'mileage_log', 'mileage_squared', 'mileage_per_year',
    
    # Target Encoding
    'Model_target_enc',
    
    # ì¹´í…Œê³ ë¦¬
    'FuelType_encoded', 'price_segment',
    
    # ìƒíƒœ
    'is_accident_free', 'inspection_score', 'is_premium_condition',
    
    # ì˜µì…˜
    *option_cols,
    'option_score', 'option_rate', 'option_weighted', 'is_full_option',
    
    # ì§€ì—­
    'is_metro',
    
    # ìƒí˜¸ì‘ìš©
    'age_option_interaction', 'model_option_interaction', 'age_mileage_interaction'
]

# Train/Test ë¶„ë¦¬ (is_train ë§ˆì»¤ ì‚¬ìš©)
train_df = df[df['is_train'] == 1].copy()
test_df = df[df['is_train'] == 0].copy()

X_train = train_df[feature_cols]
y_train = train_df['Price_log']
X_test = test_df[feature_cols]
y_test = test_df['Price_log']

print(f"âœ“ Feature: {len(feature_cols)}ê°œ")
print(f"âœ“ Train: {len(X_train):,}í–‰")
print(f"âœ“ Test: {len(X_test):,}í–‰")

# ========== 7. ëª¨ë¸ í•™ìŠµ ==========
print("\nğŸ”¥ Step 7: XGBoost í•™ìŠµ...")

model = xgb.XGBRegressor(
    n_estimators=600,
    learning_rate=0.03,
    max_depth=5,
    min_child_weight=3,
    subsample=0.7,
    colsample_bytree=0.7,
    colsample_bylevel=0.7,
    gamma=0.5,
    reg_alpha=1.0,
    reg_lambda=3.0,
    random_state=42,
    n_jobs=-1,
    verbosity=0
)

model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=100
)

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")

# ========== 8. ëª¨ë¸ í‰ê°€ ==========
print("\n" + "="*80)
print("ğŸ“ˆ Step 8: ëª¨ë¸ í‰ê°€")
print("="*80)

# ë¡œê·¸ â†’ ì›ë˜ ê°€ê²©
y_train_pred_log = model.predict(X_train)
y_test_pred_log = model.predict(X_test)

y_train_pred = np.expm1(y_train_pred_log)
y_test_pred = np.expm1(y_test_pred_log)
y_train_true = np.expm1(y_train)
y_test_true = np.expm1(y_test)

# Train ì„±ëŠ¥
train_mae = mean_absolute_error(y_train_true, y_train_pred)
train_r2 = r2_score(y_train_true, y_train_pred)
train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))

print(f"\nğŸ”µ Train ì„±ëŠ¥:")
print(f"   MAE:  {train_mae:.2f}ë§Œì›")
print(f"   RMSE: {train_rmse:.2f}ë§Œì›")
print(f"   RÂ²:   {train_r2:.4f}")

# Test ì„±ëŠ¥
test_mae = mean_absolute_error(y_test_true, y_test_pred)
test_r2 = r2_score(y_test_true, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))

print(f"\nğŸŸ¢ Test ì„±ëŠ¥:")
print(f"   MAE:  {test_mae:.2f}ë§Œì›")
print(f"   RMSE: {test_rmse:.2f}ë§Œì›")
print(f"   RÂ²:   {test_r2:.4f}")

print(f"\nğŸ“Š ê³¼ì í•© ì²´í¬:")
print(f"   Train-Test RÂ² ì°¨ì´: {train_r2 - test_r2:.4f}")
if (train_r2 - test_r2) < 0.10:
    print(f"   âœ… ê³¼ì í•© ì—†ìŒ!")
else:
    print(f"   âš ï¸ ê³¼ì í•© ì¡´ì¬")

# Feature Importance
print(f"\nâ­ Feature Importance (ìƒìœ„ 15ê°œ):")
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance.head(15).iterrows():
    print(f"   {row['feature']:35s}: {row['importance']:.4f}")

# ========== 9. ëª¨ë¸ ì €ì¥ ==========
print("\n" + "="*80)
print("ğŸ’¾ Step 9: ëª¨ë¸ ì €ì¥")
print("="*80)

os.makedirs('models', exist_ok=True)

joblib.dump(model, 'models/genesis_ultimate.pkl')
joblib.dump(encoders, 'models/genesis_ultimate_encoders.pkl')
joblib.dump(feature_cols, 'models/genesis_ultimate_features.pkl')

metrics = {
    'train_mae': train_mae, 'train_r2': train_r2, 'train_rmse': train_rmse,
    'test_mae': test_mae, 'test_r2': test_r2, 'test_rmse': test_rmse,
    'n_samples': len(df), 'n_features': len(feature_cols),
    'overfitting_gap': train_r2 - test_r2,
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}
joblib.dump(metrics, 'models/genesis_ultimate_metrics.pkl')

print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

print("\n" + "="*80)
print("ğŸ‰ ì œë„¤ì‹œìŠ¤ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("="*80)
print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"   Test RÂ²:  {test_r2:.4f}")
print(f"   Test MAE: {test_mae:.2f}ë§Œì›")
print(f"   Overfit Gap: {train_r2 - test_r2:.4f}")
print(f"   ë°ì´í„°: {len(df):,}ëŒ€")
print("="*80)
