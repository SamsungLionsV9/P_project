"""
ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬
- ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ê°•í™”
- ë‹¤ì–‘í•œ HTML êµ¬ì¡° ëŒ€ì‘
- ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 
"""

import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import quote


# í™•ìž¥ëœ í‚¤ì›Œë“œ ì‚¬ì „
POSITIVE_KEYWORDS = [
    "ë¹ ë¥´ë‹¤", "ë¹ ë¦„", "ì¡°ìš©", "ë¶€ë“œëŸ½", "ì•ˆì •ì ", "íƒ„íƒ„", "ê²¬ê³ ",
    "ê°€ì„±ë¹„", "ì €ë ´", "í•©ë¦¬ì ", "ì´ë“", "í˜œìž",
    "ì¶”ì²œ", "ë§Œì¡±", "ì¢‹ìŒ", "í›Œë¥­", "ìµœê³ ", "êµ¿", "ì¢‹ì•„", "ê´œì°®",
    "ê³„ì•½", "êµ¬ìž…", "ê²°ì •", "ì„±ê³µ", "ë“í…œ", "ìƒ€ì–´", "ì§ˆë €",
    "ê°œê¿€", "ê°“ì„±ë¹„", "ì©ë‹¤", "ì¸ì •", "ë ˆì „ë“œ",
    "ì˜ˆì˜", "ë©‹ì§€", "ê³ ê¸‰", "ì„¸ë ¨", "ì´ì˜",
    "íŽ¸í•˜", "ì¾Œì ", "ë„“", "ì‹¤ìš©",
]

NEGATIVE_KEYWORDS = [
    "ê³ ìž¥", "ê²°í•¨", "í•˜ìž", "ë¬¸ì œ", "ì´ìŠˆ", "ë¶ˆëŸ‰", "íŒŒì†",
    "í˜•íŽ¸ì—†", "ì‹¤ë§", "í›„íšŒ", "ìµœì•…", "ë³„ë¡œ", "ì•„ì‰½",
    "ë¦¬ì½œ", "íšŒìˆ˜", "ê¸‰ë°œì§„",
    "í‰ê¸°ì°¨", "í­íƒ„", "ì§€ë¢°", "ì“°ë ˆê¸°", "ë…¸ë‹µ",
    "ë¹„ì‹¸", "ë¹„ìŒˆ", "ë¹„ìš©", "ë¶€ë‹´",
    "ì‹œë„ëŸ½", "ë–¨ë¦¼", "ì†ŒìŒ", "ì§„ë™",
]

STRONG_POSITIVE = ["ìµœê³ ", "í›Œë¥­", "êµ¿", "ê°œê¿€", "ê°“ì„±ë¹„", "ë ˆì „ë“œ"]
STRONG_NEGATIVE = ["ìµœì•…", "ì“°ë ˆê¸°", "í‰ê¸°ì°¨", "í­íƒ„", "ê¸‰ë°œì§„"]


class ImprovedCommunityCollector:
    """ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.naver.com/'
        }
    
    def search_naver_blog_improved(self, car_model, limit=50):
        """
        ê°œì„ ëœ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
        - ë” ë§Žì€ HTML êµ¬ì¡° ì§€ì›
        - ìž¬ì‹œë„ ë¡œì§
        - ë” ë§Žì€ ì •ë³´ ì¶”ì¶œ
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            limit: ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            
        Returns:
            list: [{'title': '...', 'description': '...', 'date': '...', 'url': '...'}, ...]
        """
        print(f"ðŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ '{car_model}' ê²€ìƒ‰ ì¤‘ (ê°œì„  ë²„ì „)...")
        
        posts = []
        queries = [
            f"{car_model} ì¤‘ê³ ì°¨",
            f"{car_model} ë¦¬ë·°",
            f"{car_model} ì‹œìŠ¹ê¸°",
            f"{car_model} êµ¬ë§¤",
        ]
        
        for query in queries:
            if len(posts) >= limit:
                break
            
            try:
                # URL ì¸ì½”ë”©
                encoded_query = quote(query)
                url = f"https://search.naver.com/search.naver?where=blog&sm=tab_jum&query={encoded_query}"
                
                print(f"  â†’ ê²€ìƒ‰: '{query}'")
                response = requests.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‹¤ì–‘í•œ ì„ íƒìž ì‹œë„
                selectors = [
                    'div.detail_box',  # ìƒˆë¡œìš´ êµ¬ì¡°
                    'li.bx',           # ì´ì „ êµ¬ì¡°
                    'div.total_wrap',  # í†µí•© ê²€ìƒ‰
                    'div.api_subject_bx',  # API í˜•ì‹
                ]
                
                items = []
                for selector in selectors:
                    items = soup.select(selector)
                    if items:
                        print(f"    âœ“ ì„ íƒìž '{selector}'ë¡œ {len(items)}ê°œ ë°œê²¬")
                        break
                
                if not items:
                    print(f"    âš ï¸ '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    continue
                
                for item in items:
                    if len(posts) >= limit:
                        break
                    
                    try:
                        # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒìž ì‹œë„)
                        title_elem = (
                            item.select_one('a.title_link') or
                            item.select_one('a.api_txt_lines.total_tit') or
                            item.select_one('.title') or
                            item.select_one('a')
                        )
                        
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        
                        # URL ì¶”ì¶œ
                        url = title_elem.get('href', '')
                        
                        # ì„¤ëª… ì¶”ì¶œ
                        desc_elem = (
                            item.select_one('a.dsc_link') or
                            item.select_one('.dsc_txt') or
                            item.select_one('.api_txt_lines.dsc_txt') or
                            item.select_one('.sh_blog_passage')
                        )
                        description = desc_elem.get_text(strip=True) if desc_elem else ''
                        
                        # ë‚ ì§œ ì¶”ì¶œ
                        date_elem = (
                            item.select_one('.sub_time') or
                            item.select_one('.date') or
                            item.select_one('.sub_txt')
                        )
                        date = date_elem.get_text(strip=True) if date_elem else ''
                        
                        # ë¸”ë¡œê±° ì´ë¦„
                        author_elem = (
                            item.select_one('.name') or
                            item.select_one('.sub_txt.sub_name')
                        )
                        author = author_elem.get_text(strip=True) if author_elem else ''
                        
                        posts.append({
                            'title': title,
                            'description': description,
                            'date': date,
                            'author': author,
                            'url': url,
                            'source': 'ë„¤ì´ë²„ë¸”ë¡œê·¸',
                            'query': query
                        })
                        
                    except Exception as e:
                        continue
                
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì°¨ë‹¨ ë°©ì§€)
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    âœ— '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        seen_titles = set()
        unique_posts = []
        for post in posts:
            if post['title'] not in seen_titles:
                seen_titles.add(post['title'])
                unique_posts.append(post)
        
        print(f"\n  âœ“ ì´ {len(unique_posts)}ê°œ ê³ ìœ  ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return unique_posts
    
    def search_daum_cafe(self, car_model, limit=30):
        """
        ë‹¤ìŒ ì¹´íŽ˜ ê²€ìƒ‰ (ì¶”ê°€ ë°ì´í„° ì†ŒìŠ¤)
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            limit: ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            
        Returns:
            list: [{'title': '...', 'description': '...', ...}, ...]
        """
        print(f"â˜• ë‹¤ìŒ ì¹´íŽ˜ '{car_model}' ê²€ìƒ‰ ì¤‘...")
        
        posts = []
        
        try:
            query = f"{car_model} ì¤‘ê³ ì°¨"
            encoded_query = quote(query)
            url = f"https://search.daum.net/search?w=cafe&q={encoded_query}"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            items = soup.select('.item_cont')
            
            for item in items[:limit]:
                try:
                    title_elem = item.select_one('.tit_link')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    desc_elem = item.select_one('.desc_link')
                    description = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    posts.append({
                        'title': title,
                        'description': description,
                        'url': url,
                        'source': 'ë‹¤ìŒì¹´íŽ˜'
                    })
                    
                except Exception:
                    continue
            
            print(f"  âœ“ ë‹¤ìŒ ì¹´íŽ˜ì—ì„œ {len(posts)}ê°œ ìˆ˜ì§‘")
            
        except Exception as e:
            print(f"  âœ— ë‹¤ìŒ ì¹´íŽ˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return posts
    
    def analyze_sentiment_enhanced(self, posts):
        """
        í™•ìž¥ëœ í‚¤ì›Œë“œ ì‚¬ì „ìœ¼ë¡œ ê°ì„± ë¶„ì„
        
        Args:
            posts: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: {'positive_ratio': 0.6, 'score': 20, 'trend': 'positive', ...}
        """
        if not posts:
            return {
                'positive_ratio': 0.5,
                'negative_ratio': 0.5,
                'score': 0,
                'trend': 'neutral',
                'total_posts': 0,
                'analysis_detail': 'ë°ì´í„° ë¶€ì¡±'
            }
        
        pos_count = 0
        neg_count = 0
        total_score = 0
        
        keyword_hits = {
            'positive': {},
            'negative': {}
        }
        
        for post in posts:
            # ì œëª© + ì„¤ëª… í•©ì¹˜ê¸°
            text = (post.get('title', '') + ' ' + post.get('description', '')).lower()
            
            # ê°•í•œ ê¸ì • (ê°€ì¤‘ì¹˜ 2)
            for keyword in STRONG_POSITIVE:
                if keyword in text:
                    total_score += 2
                    keyword_hits['positive'][keyword] = keyword_hits['positive'].get(keyword, 0) + 1
            
            # ì¼ë°˜ ê¸ì • (ê°€ì¤‘ì¹˜ 1)
            for keyword in POSITIVE_KEYWORDS:
                if keyword in text:
                    total_score += 1
                    keyword_hits['positive'][keyword] = keyword_hits['positive'].get(keyword, 0) + 1
            
            # ê°•í•œ ë¶€ì • (ê°€ì¤‘ì¹˜ 2)
            for keyword in STRONG_NEGATIVE:
                if keyword in text:
                    total_score -= 2
                    keyword_hits['negative'][keyword] = keyword_hits['negative'].get(keyword, 0) + 1
            
            # ì¼ë°˜ ë¶€ì • (ê°€ì¤‘ì¹˜ 1)
            for keyword in NEGATIVE_KEYWORDS:
                if keyword in text:
                    total_score -= 1
                    keyword_hits['negative'][keyword] = keyword_hits['negative'].get(keyword, 0) + 1
            
            # ê²Œì‹œê¸€ë³„ íŒë‹¨
            post_has_positive = any(k in text for k in POSITIVE_KEYWORDS + STRONG_POSITIVE)
            post_has_negative = any(k in text for k in NEGATIVE_KEYWORDS + STRONG_NEGATIVE)
            
            if post_has_positive and not post_has_negative:
                pos_count += 1
            elif post_has_negative and not post_has_positive:
                neg_count += 1
        
        total = len(posts)
        pos_ratio = pos_count / total if total > 0 else 0
        neg_ratio = neg_count / total if total > 0 else 0
        neu_ratio = 1 - pos_ratio - neg_ratio
        
        # ì „ì²´ ì ìˆ˜ ì •ê·œí™” (-10 ~ +10)
        avg_score = total_score / total if total > 0 else 0
        normalized_score = max(-10, min(10, avg_score))
        
        # ì¶”ì„¸ íŒë‹¨
        if normalized_score > 3:
            trend = 'positive'
        elif normalized_score < -3:
            trend = 'negative'
        else:
            trend = 'neutral'
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_positive = sorted(keyword_hits['positive'].items(), key=lambda x: x[1], reverse=True)[:5]
        top_negative = sorted(keyword_hits['negative'].items(), key=lambda x: x[1], reverse=True)[:5]
        
        result = {
            'positive_ratio': round(pos_ratio, 2),
            'negative_ratio': round(neg_ratio, 2),
            'neutral_ratio': round(neu_ratio, 2),
            'score': round(normalized_score, 1),
            'trend': trend,
            'total_posts': total,
            'top_positive_keywords': [f"{k} ({v}ê±´)" for k, v in top_positive],
            'top_negative_keywords': [f"{k} ({v}ê±´)" for k, v in top_negative],
            'keyword_matches': {
                'positive': sum(keyword_hits['positive'].values()),
                'negative': sum(keyword_hits['negative'].values())
            }
        }
        
        print(f"\nðŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼:")
        print(f"  ê¸ì •: {result['positive_ratio']:.0%} | ë¶€ì •: {result['negative_ratio']:.0%} | ì¤‘ë¦½: {result['neutral_ratio']:.0%}")
        print(f"  ì ìˆ˜: {result['score']:.1f}/10 ({result['trend']})")
        
        if top_positive:
            print(f"  ì£¼ìš” ê¸ì • í‚¤ì›Œë“œ: {', '.join([k for k, _ in top_positive[:3]])}")
        if top_negative:
            print(f"  ì£¼ìš” ë¶€ì • í‚¤ì›Œë“œ: {', '.join([k for k, _ in top_negative[:3]])}")
        
        return result
    
    def collect_all_community_data(self, car_model, limit=50):
        """
        ëª¨ë“  ì»¤ë®¤ë‹ˆí‹° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            limit: ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            
        Returns:
            dict: {'posts': [...], 'sentiment': {...}}
        """
        print("=" * 80)
        print(f"ðŸ’¬ '{car_model}' ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ (ë©€í‹° ì†ŒìŠ¤)")
        print("=" * 80)
        
        all_posts = []
        
        # 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸
        naver_posts = self.search_naver_blog_improved(car_model, limit=limit)
        all_posts.extend(naver_posts)
        
        print()
        
        # 2. ë‹¤ìŒ ì¹´íŽ˜ (ì¶”ê°€)
        if len(all_posts) < limit:
            daum_posts = self.search_daum_cafe(car_model, limit=min(30, limit - len(all_posts)))
            all_posts.extend(daum_posts)
        
        print()
        
        # 3. ê°ì„± ë¶„ì„
        sentiment = self.analyze_sentiment_enhanced(all_posts)
        
        print(f"\nâœ… ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        print("=" * 80)
        
        return {
            'posts': all_posts,
            'sentiment': sentiment,
            'post_count': len(all_posts),
            'sources': list(set(p.get('source', 'unknown') for p in all_posts))
        }


if __name__ == "__main__":
    print("=" * 80)
    print("ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸
    collector = ImprovedCommunityCollector()
    
    test_models = ["ê·¸ëžœì €", "ì•„ë°˜ë–¼", "K5"]
    
    for model in test_models[:1]:  # ì¼ë‹¨ í•˜ë‚˜ë§Œ
        print(f"\n{'='*80}")
        print(f"í…ŒìŠ¤íŠ¸: {model}")
        print(f"{'='*80}\n")
        
        result = collector.collect_all_community_data(model, limit=50)
        
        print(f"\nðŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"  ìˆ˜ì§‘ ê²Œì‹œê¸€: {result['post_count']}ê°œ")
        print(f"  ë°ì´í„° ì†ŒìŠ¤: {', '.join(result['sources'])}")
        print(f"  ê°ì„± ì ìˆ˜: {result['sentiment']['score']:.1f}/10")
        print(f"  ì¶”ì„¸: {result['sentiment']['trend']}")
        
        # ìƒ˜í”Œ ê²Œì‹œê¸€ ì¶œë ¥
        if result['posts']:
            print(f"\nðŸ“ ìƒ˜í”Œ ê²Œì‹œê¸€ (ìƒìœ„ 5ê°œ):")
            for i, post in enumerate(result['posts'][:5], 1):
                print(f"\n  {i}. {post['title']}")
                if post.get('description'):
                    desc = post['description'][:80] + '...' if len(post['description']) > 80 else post['description']
                    print(f"     {desc}")
                print(f"     ì¶œì²˜: {post.get('source', 'unknown')}")
