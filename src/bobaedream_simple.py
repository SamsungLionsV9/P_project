"""
ë³´ë°°ë“œë¦¼ ê°„ë‹¨ í¬ë¡¤ëŸ¬ - requestsë§Œ ì‚¬ìš©
Selenium ì—†ì´ ê³µê°œ ê²Œì‹œíŒë§Œ í¬ë¡¤ë§
"""

import requests
from bs4 import BeautifulSoup
import time
import re


POSITIVE_KEYWORDS = [
    "ë¹ ë¥´ë‹¤", "ì¡°ìš©", "ë¶€ë“œëŸ½", "ì•ˆì •ì ", "íƒ„íƒ„",
    "ê°€ì„±ë¹„", "ì €ë ´", "í•©ë¦¬ì ", "í˜œì",
    "ì¶”ì²œ", "ë§Œì¡±", "ì¢‹ìŒ", "í›Œë¥­", "ìµœê³ ", "êµ¿", "ê´œì°®",
    "ê³„ì•½", "êµ¬ì…", "ìƒ€ì–´",
    "ì˜ˆì˜", "ë©‹ì§€", "ê³ ê¸‰",
    "í¸í•˜", "ì¾Œì ", "ë„“", "ì‹¤ìš©",
]

NEGATIVE_KEYWORDS = [
    "ê³ ì¥", "ê²°í•¨", "í•˜ì", "ë¬¸ì œ", "ì´ìŠˆ", "ë¶ˆëŸ‰",
    "ì‹¤ë§", "í›„íšŒ", "ìµœì•…", "ë³„ë¡œ", "ì•„ì‰½",
    "ë¦¬ì½œ", "ê¸‰ë°œì§„",
    "í‰ê¸°ì°¨", "í­íƒ„", "ì“°ë ˆê¸°",
    "ë¹„ì‹¸", "ë¶€ë‹´",
    "ì‹œë„ëŸ½", "ë–¨ë¦¼", "ì†ŒìŒ",
]


class SimpleBobaedreamCrawler:
    """requests ê¸°ë°˜ ê°„ë‹¨ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def scrape_free_board(self, car_model, pages=10):
        """
        ììœ ê²Œì‹œíŒì—ì„œ ì°¨ëŸ‰ ê´€ë ¨ ê¸€ ìˆ˜ì§‘
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            pages: í˜ì´ì§€ ìˆ˜
            
        Returns:
            list: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸš— ë³´ë°°ë“œë¦¼ ììœ ê²Œì‹œíŒ '{car_model}' ìˆ˜ì§‘ ì¤‘...")
        
        posts = []
        
        for page in range(1, pages + 1):
            try:
                # ììœ ê²Œì‹œíŒ URL
                url = f"https://www.bobaedream.co.kr/list?code=free&page={page}"
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
                links = soup.find_all('a', href=re.compile(r'/view/'))
                
                page_posts = 0
                for link in links:
                    title = link.get_text(strip=True)
                    
                    # ì°¨ëŸ‰ëª… í•„í„°ë§
                    if car_model.lower() not in title.lower():
                        continue
                    
                    if len(title) < 5:
                        continue
                    
                    url = link.get('href', '')
                    if url and not url.startswith('http'):
                        url = 'https://www.bobaedream.co.kr' + url
                    
                    posts.append({
                        'title': title,
                        'url': url,
                        'source': 'ë³´ë°°ë“œë¦¼-ììœ ê²Œì‹œíŒ'
                    })
                    page_posts += 1
                
                if page_posts > 0:
                    print(f"  âœ“ í˜ì´ì§€ {page}: {page_posts}ê°œ")
                
                time.sleep(0.5)  # ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                print(f"  âš ï¸ í˜ì´ì§€ {page} ì‹¤íŒ¨: {e}")
                continue
        
        print(f"  âœ“ ì´ {len(posts)}ê°œ ìˆ˜ì§‘")
        return posts
    
    def scrape_humor_board(self, car_model, pages=10):
        """
        ìœ ë¨¸ê²Œì‹œíŒ (ì¸ê¸°ê¸€)
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            pages: í˜ì´ì§€ ìˆ˜
            
        Returns:
            list: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸš— ë³´ë°°ë“œë¦¼ ìœ ë¨¸ê²Œì‹œíŒ '{car_model}' ìˆ˜ì§‘ ì¤‘...")
        
        posts = []
        
        for page in range(1, pages + 1):
            try:
                url = f"https://www.bobaedream.co.kr/list?code=humor&page={page}"
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                links = soup.find_all('a', href=re.compile(r'/view/'))
                
                page_posts = 0
                for link in links:
                    title = link.get_text(strip=True)
                    
                    if car_model.lower() not in title.lower():
                        continue
                    
                    if len(title) < 5:
                        continue
                    
                    url = link.get('href', '')
                    if url and not url.startswith('http'):
                        url = 'https://www.bobaedream.co.kr' + url
                    
                    posts.append({
                        'title': title,
                        'url': url,
                        'source': 'ë³´ë°°ë“œë¦¼-ìœ ë¨¸ê²Œì‹œíŒ'
                    })
                    page_posts += 1
                
                if page_posts > 0:
                    print(f"  âœ“ í˜ì´ì§€ {page}: {page_posts}ê°œ")
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"  âš ï¸ í˜ì´ì§€ {page} ì‹¤íŒ¨: {e}")
                continue
        
        print(f"  âœ“ ì´ {len(posts)}ê°œ ìˆ˜ì§‘")
        return posts
    
    def analyze_sentiment(self, posts):
        """ê°ì„± ë¶„ì„"""
        if not posts:
            return {
                'score': 0,
                'positive_ratio': 0.5,
                'negative_ratio': 0.5,
                'neutral_ratio': 0.0,
                'trend': 'neutral',
                'total_posts': 0,
                'source': 'bobaedream_simple'
            }
        
        pos_count = 0
        neg_count = 0
        total_score = 0
        
        for post in posts:
            text = post.get('title', '').lower()
            
            pos_score = sum(1 for w in POSITIVE_KEYWORDS if w in text)
            neg_score = sum(1 for w in NEGATIVE_KEYWORDS if w in text)
            
            post_score = pos_score - neg_score
            total_score += post_score
            
            if post_score > 0:
                pos_count += 1
            elif post_score < 0:
                neg_count += 1
        
        total = len(posts)
        pos_ratio = pos_count / total if total > 0 else 0
        neg_ratio = neg_count / total if total > 0 else 0
        neu_ratio = 1 - pos_ratio - neg_ratio
        
        avg_score = total_score / total if total > 0 else 0
        normalized_score = max(-10, min(10, avg_score))
        
        if normalized_score > 2:
            trend = 'positive'
        elif normalized_score < -2:
            trend = 'negative'
        else:
            trend = 'neutral'
        
        result = {
            'score': round(normalized_score, 1),
            'positive_ratio': round(pos_ratio, 2),
            'negative_ratio': round(neg_ratio, 2),
            'neutral_ratio': round(neu_ratio, 2),
            'trend': trend,
            'total_posts': total,
            'source': 'bobaedream_simple'
        }
        
        print(f"\nğŸ“Š ê°ì„± ë¶„ì„:")
        print(f"  ê¸ì •: {result['positive_ratio']:.0%} | ë¶€ì •: {result['negative_ratio']:.0%}")
        print(f"  ì ìˆ˜: {result['score']:.1f}/10 ({result['trend']})")
        
        return result
    
    def collect_all(self, car_model, free_pages=10, humor_pages=5):
        """
        í†µí•© ìˆ˜ì§‘ + ê°ì„± ë¶„ì„
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            free_pages: ììœ ê²Œì‹œíŒ í˜ì´ì§€ ìˆ˜
            humor_pages: ìœ ë¨¸ê²Œì‹œíŒ í˜ì´ì§€ ìˆ˜
            
        Returns:
            dict: {'posts': [...], 'sentiment': {...}, 'post_count': int}
        """
        print("=" * 80)
        print(f"ğŸš— ë³´ë°°ë“œë¦¼ '{car_model}' ê°„ë‹¨ í¬ë¡¤ë§")
        print("=" * 80)
        
        all_posts = []
        
        # ììœ ê²Œì‹œíŒ
        posts1 = self.scrape_free_board(car_model, pages=free_pages)
        all_posts.extend(posts1)
        
        # ìœ ë¨¸ê²Œì‹œíŒ
        posts2 = self.scrape_humor_board(car_model, pages=humor_pages)
        all_posts.extend(posts2)
        
        # ì¤‘ë³µ ì œê±°
        seen_titles = set()
        unique_posts = []
        for post in all_posts:
            if post['title'] not in seen_titles:
                seen_titles.add(post['title'])
                unique_posts.append(post)
        
        print(f"\nâœ… ì´ {len(unique_posts)}ê°œ ê³ ìœ  ê²Œì‹œê¸€")
        
        # ê°ì„± ë¶„ì„
        sentiment = self.analyze_sentiment(unique_posts)
        
        print("=" * 80)
        
        return {
            'posts': unique_posts,
            'sentiment': sentiment,
            'post_count': len(unique_posts)
        }


if __name__ == "__main__":
    print("=" * 80)
    print("ë³´ë°°ë“œë¦¼ ê°„ë‹¨ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    crawler = SimpleBobaedreamCrawler()
    
    # í…ŒìŠ¤íŠ¸
    result = crawler.collect_all("ê·¸ëœì €", free_pages=10, humor_pages=5)
    
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  ìˆ˜ì§‘: {result['post_count']}ê°œ")
    print(f"  ì ìˆ˜: {result['sentiment']['score']:.1f}/10")
    print(f"  ì¶”ì„¸: {result['sentiment']['trend']}")
    
    if result['posts']:
        print(f"\nğŸ“ ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):")
        for i, post in enumerate(result['posts'][:5], 1):
            print(f"\n  {i}. {post['title']}")
            print(f"     ì¶œì²˜: {post['source']}")
