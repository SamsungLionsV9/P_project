"""
ë³´ë°°ë“œë¦¼ ì‹¤ì œ í¬ë¡¤ëŸ¬ (Selenium ì‚¬ìš©)
- ì‹¤ì œ ë¸Œë¼ìš°ì €ë¡œ ì ‘ê·¼í•˜ì—¬ ë´‡ ì°¨ë‹¨ ìš°íšŒ
- ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
- ê°ì„± ë¶„ì„ ì ìš©
"""

import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import re

# í‚¤ì›Œë“œ ì‚¬ì „
POSITIVE_KEYWORDS = [
    "ë¹ ë¥´ë‹¤", "ë¹ ë¦„", "ì¡°ìš©", "ë¶€ë“œëŸ½", "ì•ˆì •ì ", "íƒ„íƒ„", "ê²¬ê³ ",
    "ê°€ì„±ë¹„", "ì €ë ´", "í•©ë¦¬ì ", "ì´ë“", "í˜œì",
    "ì¶”ì²œ", "ë§Œì¡±", "ì¢‹ìŒ", "í›Œë¥­", "ìµœê³ ", "êµ¿", "ì¢‹ì•„", "ê´œì°®",
    "ê³„ì•½", "êµ¬ì…", "ê²°ì •", "ì„±ê³µ", "ë“í…œ", "ìƒ€ì–´", "ì§ˆë €",
    "ê°œê¿€", "ê°“ì„±ë¹„", "ì©ë‹¤", "ì¸ì •", "ë ˆì „ë“œ",
    "ì˜ˆì˜", "ë©‹ì§€", "ê³ ê¸‰", "ì„¸ë ¨", "ì´ì˜",
    "í¸í•˜", "ì¾Œì ", "ë„“", "ì‹¤ìš©",
]

NEGATIVE_KEYWORDS = [
    "ê³ ì¥", "ê²°í•¨", "í•˜ì", "ë¬¸ì œ", "ì´ìŠˆ", "ë¶ˆëŸ‰", "íŒŒì†",
    "í˜•í¸ì—†", "ì‹¤ë§", "í›„íšŒ", "ìµœì•…", "ë³„ë¡œ", "ì•„ì‰½",
    "ë¦¬ì½œ", "íšŒìˆ˜", "ê¸‰ë°œì§„",
    "í‰ê¸°ì°¨", "í­íƒ„", "ì§€ë¢°", "ì“°ë ˆê¸°", "ë…¸ë‹µ",
    "ë¹„ì‹¸", "ë¹„ìŒˆ", "ë¹„ìš©", "ë¶€ë‹´",
    "ì‹œë„ëŸ½", "ë–¨ë¦¼", "ì†ŒìŒ", "ì§„ë™",
]

STRONG_POSITIVE = ["ìµœê³ ", "í›Œë¥­", "êµ¿", "ê°œê¿€", "ê°“ì„±ë¹„", "ë ˆì „ë“œ"]
STRONG_NEGATIVE = ["ìµœì•…", "ì“°ë ˆê¸°", "í‰ê¸°ì°¨", "í­íƒ„", "ê¸‰ë°œì§„"]


class BobaedreamScraper:
    """ë³´ë°°ë“œë¦¼ Selenium í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=True):
        """
        Args:
            headless: ë¸Œë¼ìš°ì € ì°½ì„ ìˆ¨ê¸¸ì§€ ì—¬ë¶€ (True=ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        """
        self.headless = headless
        self.driver = None
    
    def _init_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        if self.driver:
            return
        
        print("ğŸŒ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument('--headless')
        
        # ë´‡ ê°ì§€ ìš°íšŒ ì˜µì…˜
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        
        # ChromeDriver ìë™ ì„¤ì¹˜ ë° ì‹¤í–‰
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        
        print("  âœ“ ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ")
    
    def scrape_bobaedream(self, car_model, limit=50):
        """
        ë³´ë°°ë“œë¦¼ì—ì„œ ì‹¤ì œ ê²Œì‹œê¸€ í¬ë¡¤ë§
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            limit: ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            
        Returns:
            list: [{'title': '...', 'content': '...', 'date': '...', 'url': '...'}, ...]
        """
        print(f"ğŸš— ë³´ë°°ë“œë¦¼ '{car_model}' ê²€ìƒ‰ ì¤‘ (Selenium)...")
        
        try:
            self._init_driver()
            
            posts = []
            
            # ë³´ë°°ë“œë¦¼ í†µí•© ê²€ìƒ‰ (ì¤‘ê³ ì°¨ ê²Œì‹œíŒ ìœ„ì£¼)
            search_url = f"https://www.bobaedream.co.kr/cyber/CyberCont.php?gubun=I&page=1&search_flag=Y&search_sel=I&search_txt={car_model}"
            
            print(f"  â†’ URL ì ‘ì† ì¤‘...")
            self.driver.get(search_url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì°¾ê¸° (ë³´ë°°ë“œë¦¼ êµ¬ì¡°ì— ë§ê²Œ)
            # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
            selectors = [
                'div.list',
                'table.bbsList',
                'tr.pl',
                'div.bulletin-list',
                'li.list-item'
            ]
            
            items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    print(f"    âœ“ '{selector}' ì„ íƒìë¡œ {len(items)}ê°œ ë°œê²¬")
                    break
            
            if not items:
                # ì§ì ‘ ë§í¬ë¡œ ì‹œë„
                items = soup.find_all('a', href=re.compile(r'view\.php|idx='))
                print(f"    âœ“ ë§í¬ ê¸°ë°˜ìœ¼ë¡œ {len(items)}ê°œ ë°œê²¬")
            
            for item in items[:limit]:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.find('a') if hasattr(item, 'find') else item
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    
                    # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                    if len(title) < 5:
                        continue
                    
                    # URL ì¶”ì¶œ
                    url = title_elem.get('href', '')
                    if url and not url.startswith('http'):
                        url = 'https://www.bobaedream.co.kr' + url
                    
                    # ë‚ ì§œ ì¶”ì¶œ (ê°€ëŠ¥í•˜ë©´)
                    date_elem = item.find(class_=re.compile(r'date|time'))
                    date = date_elem.get_text(strip=True) if date_elem else ''
                    
                    posts.append({
                        'title': title,
                        'content': '',  # ëª©ë¡ì—ì„œëŠ” ë‚´ìš© ì—†ìŒ
                        'date': date,
                        'url': url,
                        'source': 'ë³´ë°°ë“œë¦¼'
                    })
                    
                except Exception as e:
                    continue
            
            print(f"  âœ“ ë³´ë°°ë“œë¦¼ì—ì„œ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
            
            return posts
            
        except Exception as e:
            print(f"  âœ— ë³´ë°°ë“œë¦¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
        
        finally:
            # ë“œë¼ì´ë²„ëŠ” ì¬ì‚¬ìš©ì„ ìœ„í•´ ë‹«ì§€ ì•ŠìŒ (close()ì—ì„œ ì²˜ë¦¬)
            pass
    
    def scrape_bobaedream_usedcar_board(self, car_model, limit=30):
        """
        ë³´ë°°ë“œë¦¼ ì¤‘ê³ ì°¨ ê²Œì‹œíŒ ì§ì ‘ í¬ë¡¤ë§
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            limit: ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            
        Returns:
            list: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸš— ë³´ë°°ë“œë¦¼ ì¤‘ê³ ì°¨ ê²Œì‹œíŒ '{car_model}' ê²€ìƒ‰ ì¤‘...")
        
        try:
            self._init_driver()
            
            posts = []
            
            # ì¤‘ê³ ì°¨ ë§¤ë¬¼ ê²Œì‹œíŒ
            board_url = "https://www.bobaedream.co.kr/cyber/CyberCont.php?gubun=K"
            
            self.driver.get(board_url)
            time.sleep(2)
            
            # ê²€ìƒ‰ì°½ì— ì°¨ëŸ‰ëª… ì…ë ¥
            try:
                search_input = self.driver.find_element(By.NAME, "search_txt")
                search_input.clear()
                search_input.send_keys(car_model)
                
                # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
                search_btn = self.driver.find_element(By.CSS_SELECTOR, "button[type='submit']")
                search_btn.click()
                
                time.sleep(3)
                
            except:
                print("  âš ï¸ ê²€ìƒ‰ì°½ ì‚¬ìš© ë¶ˆê°€, ì§ì ‘ ê²€ìƒ‰ URL ì‚¬ìš©")
            
            # ê²°ê³¼ íŒŒì‹±
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ì œëª©ì—ì„œ ì°¨ëŸ‰ëª… í¬í•¨ëœ ê²ƒë§Œ ì¶”ì¶œ
            links = soup.find_all('a', href=True)
            
            for link in links[:limit]:
                try:
                    title = link.get_text(strip=True)
                    
                    # ì°¨ëŸ‰ëª…ì´ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if car_model.lower() not in title.lower():
                        continue
                    
                    if len(title) < 5:
                        continue
                    
                    url = link.get('href', '')
                    if url and not url.startswith('http'):
                        url = 'https://www.bobaedream.co.kr' + url
                    
                    posts.append({
                        'title': title,
                        'content': '',
                        'url': url,
                        'source': 'ë³´ë°°ë“œë¦¼-ì¤‘ê³ ì°¨ê²Œì‹œíŒ'
                    })
                    
                except:
                    continue
            
            print(f"  âœ“ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
            
            return posts
            
        except Exception as e:
            print(f"  âœ— ì¤‘ê³ ì°¨ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_sentiment(self, posts):
        """
        ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ê°ì„± ë¶„ì„
        
        Args:
            posts: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: ê°ì„± ë¶„ì„ ê²°ê³¼
        """
        if not posts:
            return {
                'positive_ratio': 0.5,
                'negative_ratio': 0.5,
                'score': 0,
                'trend': 'neutral',
                'total_posts': 0
            }
        
        pos_count = 0
        neg_count = 0
        total_score = 0
        
        for post in posts:
            text = (post.get('title', '') + ' ' + post.get('content', '')).lower()
            
            # ê°•í•œ ê¸ì • (ê°€ì¤‘ì¹˜ 2)
            strong_pos = sum(2 for w in STRONG_POSITIVE if w in text)
            # ì¼ë°˜ ê¸ì • (ê°€ì¤‘ì¹˜ 1)
            normal_pos = sum(1 for w in POSITIVE_KEYWORDS if w in text)
            
            # ê°•í•œ ë¶€ì • (ê°€ì¤‘ì¹˜ 2)
            strong_neg = sum(2 for w in STRONG_NEGATIVE if w in text)
            # ì¼ë°˜ ë¶€ì • (ê°€ì¤‘ì¹˜ 1)
            normal_neg = sum(1 for w in NEGATIVE_KEYWORDS if w in text)
            
            post_score = (strong_pos + normal_pos) - (strong_neg + normal_neg)
            total_score += post_score
            
            if post_score > 0:
                pos_count += 1
            elif post_score < 0:
                neg_count += 1
        
        total = len(posts)
        pos_ratio = pos_count / total if total > 0 else 0
        neg_ratio = neg_count / total if total > 0 else 0
        neu_ratio = 1 - pos_ratio - neg_ratio
        
        # ì ìˆ˜ ì •ê·œí™” (-10 ~ +10)
        avg_score = total_score / total if total > 0 else 0
        normalized_score = max(-10, min(10, avg_score))
        
        # ì¶”ì„¸ íŒë‹¨
        if normalized_score > 3:
            trend = 'positive'
        elif normalized_score < -3:
            trend = 'negative'
        else:
            trend = 'neutral'
        
        result = {
            'positive_ratio': round(pos_ratio, 2),
            'negative_ratio': round(neg_ratio, 2),
            'neutral_ratio': round(neu_ratio, 2),
            'score': round(normalized_score, 1),
            'trend': trend,
            'total_posts': total
        }
        
        print(f"\nğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼:")
        print(f"  ê¸ì •: {result['positive_ratio']:.0%} | ë¶€ì •: {result['negative_ratio']:.0%} | ì¤‘ë¦½: {result['neutral_ratio']:.0%}")
        print(f"  ì ìˆ˜: {result['score']:.1f}/10 ({result['trend']})")
        
        return result
    
    def collect_all(self, car_model, limit=50):
        """
        ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ + ê°ì„± ë¶„ì„
        
        Args:
            car_model: ì°¨ëŸ‰ ëª¨ë¸ëª…
            limit: ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            
        Returns:
            dict: {'posts': [...], 'sentiment': {...}}
        """
        print("=" * 80)
        print(f"ğŸš— ë³´ë°°ë“œë¦¼ '{car_model}' ë°ì´í„° ìˆ˜ì§‘ (Selenium)")
        print("=" * 80)
        
        all_posts = []
        
        # ë°©ë²• 1: í†µí•© ê²€ìƒ‰
        posts1 = self.scrape_bobaedream(car_model, limit=limit//2)
        all_posts.extend(posts1)
        
        # ë°©ë²• 2: ì¤‘ê³ ì°¨ ê²Œì‹œíŒ
        if len(all_posts) < limit:
            posts2 = self.scrape_bobaedream_usedcar_board(car_model, limit=limit//2)
            all_posts.extend(posts2)
        
        # ì¤‘ë³µ ì œê±°
        seen_titles = set()
        unique_posts = []
        for post in all_posts:
            if post['title'] not in seen_titles:
                seen_titles.add(post['title'])
                unique_posts.append(post)
        
        print(f"\nâœ… ì´ {len(unique_posts)}ê°œ ê³ ìœ  ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
        # ê°ì„± ë¶„ì„
        sentiment = self.analyze_sentiment(unique_posts)
        
        print("=" * 80)
        
        return {
            'posts': unique_posts,
            'sentiment': sentiment,
            'post_count': len(unique_posts)
        }
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            print("âœ“ ë¸Œë¼ìš°ì € ì¢…ë£Œ")


if __name__ == "__main__":
    print("=" * 80)
    print("ë³´ë°°ë“œë¦¼ Selenium í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    scraper = BobaedreamScraper(headless=True)
    
    try:
        # í…ŒìŠ¤íŠ¸
        result = scraper.collect_all("ê·¸ëœì €", limit=50)
        
        print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"  ìˆ˜ì§‘ ê²Œì‹œê¸€: {result['post_count']}ê°œ")
        print(f"  ê°ì„± ì ìˆ˜: {result['sentiment']['score']:.1f}/10")
        print(f"  ì¶”ì„¸: {result['sentiment']['trend']}")
        
        if result['posts']:
            print(f"\nğŸ“ ìƒ˜í”Œ ê²Œì‹œê¸€ (ìƒìœ„ 5ê°œ):")
            for i, post in enumerate(result['posts'][:5], 1):
                print(f"\n  {i}. {post['title']}")
                print(f"     ì¶œì²˜: {post['source']}")
        
    finally:
        scraper.close()
