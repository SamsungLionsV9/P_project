"""
êµ­ì‚°ì°¨ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ - ìµœì¢… ì†”ë£¨ì…˜
1. Target Encoding (ëª¨ë¸ë³„ í‰ê·  ê°€ê²©)
2. ê°•ë ¥í•œ ì •ê·œí™”
3. ê°€ê²© ë¡œê·¸ ë³€í™˜
4. Stratified sampling (ê°€ê²©ëŒ€ë³„)
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import xgboost as xgb
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ğŸš— êµ­ì‚°ì°¨ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ - ìµœì¢… ì†”ë£¨ì…˜ (Target Encoding)")
print("="*80)
print()

# ========== 1. ë°ì´í„° ë¡œë“œ ==========
print("ğŸ“‚ Step 1: ë°ì´í„° ë¡œë“œ...")

df_raw = pd.read_csv('encar_raw_domestic.csv')
df_detail = pd.read_csv('data/complete_domestic_details.csv')
df = df_raw.merge(df_detail, left_on='Id', right_on='car_id', how='inner')

genesis_keywords = ['ì œë„¤ì‹œìŠ¤', 'GENESIS', 'Genesis']
df = df[~df['Manufacturer'].str.contains('|'.join(genesis_keywords), case=False, na=False)]
print(f"âœ“ ë°ì´í„°: {len(df):,}í–‰")

# ========== 2. ì „ì²˜ë¦¬ ==========
print("\nğŸ”§ Step 2: ì „ì²˜ë¦¬...")

df = df.dropna(subset=['Price', 'Mileage', 'Year', 'Manufacturer', 'Model'])
df = df[df['Price'] > 100]
df = df[df['Price'] < 12000]  # 1.2ì–µ ì´í•˜
df = df[df['Mileage'] < 350000]
df = df[df['Year'] >= 2008]

# ë¡œê·¸ ë³€í™˜ (í•µì‹¬!)
df['Price_log'] = np.log1p(df['Price'])
print(f"âœ“ ì „ì²˜ë¦¬ í›„: {len(df):,}í–‰")

# ========== 3. Target Encoding (í•µì‹¬!) ==========
print("\nâ­ Step 3: Target Encoding...")

# Train/Test ë¶„ë¦¬ (ë¨¼ì €!)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# ë¶„ë¦¬ ë§ˆì»¤ ì¶”ê°€
train_df['is_train'] = 1
test_df['is_train'] = 0

# Target Encodingì„ Trainì—ì„œë§Œ í•™ìŠµ
def create_target_encoding(train, test, col, target='Price_log', min_samples=20):
    """Target Encoding with smoothing"""
    # ì „ì²´ í‰ê· 
    global_mean = train[target].mean()
    
    # ê° ì¹´í…Œê³ ë¦¬ì˜ í‰ê· ê³¼ ê°œìˆ˜
    agg = train.groupby(col)[target].agg(['mean', 'count'])
    counts = agg['count']
    means = agg['mean']
    
    # Smoothing (ë°ì´í„° ì ìœ¼ë©´ ì „ì²´ í‰ê· ì— ê°€ê¹ê²Œ)
    smooth = 1 / (1 + np.exp(-(counts - min_samples) / 10))
    encoded = global_mean * (1 - smooth) + means * smooth
    
    # Train ì ìš©
    train[f'{col}_target_enc'] = train[col].map(encoded).fillna(global_mean)
    
    # Test ì ìš© (Trainì—ì„œ í•™ìŠµí•œ ê°’ ì‚¬ìš©)
    test[f'{col}_target_enc'] = test[col].map(encoded).fillna(global_mean)
    
    return train, test, encoded

# Model Target Encoding (ê°€ì¥ ì¤‘ìš”!)
train_df, test_df, model_encoding = create_target_encoding(
    train_df, test_df, 'Model', 'Price_log', min_samples=30
)
print(f"âœ“ Model Target Encoding: í‰ê·  ê°€ê²©ìœ¼ë¡œ ë³€í™˜")

# Manufacturer Target Encoding
train_df, test_df, brand_encoding = create_target_encoding(
    train_df, test_df, 'Manufacturer', 'Price_log', min_samples=100
)
print(f"âœ“ Manufacturer Target Encoding")

# ë°ì´í„° ë‹¤ì‹œ í•©ì¹˜ê¸° (Feature Engineeringìš©)
df = pd.concat([train_df, test_df], ignore_index=True)

# ========== 4. Feature Engineering ==========
print("\nâš™ï¸ Step 4: Feature Engineering...")

current_year = 2025
df['age'] = current_year - df['Year']
df['age_squared'] = df['age'] ** 2
df['age_cubed'] = df['age'] ** 3

# Mileage
df['mileage_per_year'] = df['Mileage'] / (df['age'] + 1)
df['mileage_log'] = np.log1p(df['Mileage'])
df['mileage_squared'] = df['Mileage'] ** 2

# ì£¼í–‰ê±°ë¦¬ ìƒíƒœ
df['mileage_condition'] = pd.cut(
    df['mileage_per_year'],
    bins=[0, 8000, 15000, 25000, 1000000],
    labels=['excellent', 'good', 'average', 'high']
)

# ì˜µì…˜
option_cols = [
    'has_sunroof', 'has_navigation', 'has_leather_seat', 'has_smart_key', 
    'has_rear_camera', 'has_led_lamp', 'has_parking_sensor', 'has_auto_ac',
    'has_heated_seat', 'has_ventilated_seat'
]

for col in option_cols:
    df[col] = df[col].fillna(0)

df['option_score'] = df[option_cols].sum(axis=1)
df['option_rate'] = df['option_score'] / 10  # ì •ê·œí™”

# í”„ë¦¬ë¯¸ì—„ ì˜µì…˜ ê°€ì¤‘ì¹˜
premium_weights = {
    'has_sunroof': 1.5,
    'has_ventilated_seat': 1.5,
    'has_led_lamp': 1.2,
    'has_leather_seat': 1.3,
    'has_navigation': 1.1,
    'has_smart_key': 1.0,
    'has_rear_camera': 1.0,
    'has_parking_sensor': 1.0,
    'has_auto_ac': 1.0,
    'has_heated_seat': 1.0
}

df['option_weighted'] = sum(df[col] * weight for col, weight in premium_weights.items())

# ì„±ëŠ¥ ë“±ê¸‰
grade_map = {'excellent': 3, 'good': 2, 'normal': 1}
df['inspection_score'] = df['inspection_grade'].map(grade_map).fillna(1)

# ì™„ë²½í•œ ì¡°ê±´ (ë¬´ì‚¬ê³  + ìš°ìˆ˜ + ì €ì£¼í–‰)
df['is_premium_condition'] = (
    (df['is_accident_free'] == 1) & 
    (df['inspection_score'] == 3) &
    (df['mileage_per_year'] < 10000)
).astype(int)

# ì§€ì—­
df['region'] = df['region'].fillna('Unknown')
df['is_metro'] = (
    (df['region'].str.contains('ì„œìš¸')) | 
    (df['region'].str.contains('ê²½ê¸°'))
).astype(int)

# ì—°ë£Œ íƒ€ì… (ì „ê¸°/í•˜ì´ë¸Œë¦¬ë“œ í”„ë¦¬ë¯¸ì—„)
df['is_eco_fuel'] = (
    (df['FuelType'].str.contains('ì „ê¸°|í•˜ì´ë¸Œë¦¬ë“œ|LPG', case=False, na=False))
).astype(int)

# ìƒí˜¸ì‘ìš©
df['age_option_interaction'] = df['age'] * df['option_rate']
df['age_mileage_interaction'] = df['age'] * df['mileage_log']
df['model_option_interaction'] = df['Model_target_enc'] * df['option_weighted']

# ê°€ê²© êµ¬ê°„ (Frequency Encoding)
price_bins = pd.qcut(df['Price'], q=10, labels=False, duplicates='drop')
df['price_segment'] = price_bins

print(f"âœ“ Feature Engineering ì™„ë£Œ")

# ========== 5. Label Encoding ==========
print("\nğŸ·ï¸ Step 5: ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©...")

encoders = {}
for col in ['FuelType', 'mileage_condition']:
    if col in df.columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
        encoders[col] = le

# Target Encodingë„ ì €ì¥
encoders['Model_target_enc'] = model_encoding
encoders['Manufacturer_target_enc'] = brand_encoding

# ========== 6. í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ==========
print("\nğŸ“Š Step 6: í•™ìŠµ ë°ì´í„° ì¤€ë¹„...")

feature_cols = [
    # ê¸°ë³¸
    'Year', 'age', 'age_squared', 'age_cubed',
    
    # ì£¼í–‰ê±°ë¦¬
    'Mileage', 'mileage_log', 'mileage_squared', 'mileage_per_year',
    
    # Target Encoding (í•µì‹¬!)
    'Model_target_enc', 'Manufacturer_target_enc',
    
    # ì¹´í…Œê³ ë¦¬
    'FuelType_encoded', 'mileage_condition_encoded', 
    'price_segment', 'is_eco_fuel',
    
    # ìƒíƒœ
    'is_accident_free', 'inspection_score', 'is_premium_condition',
    
    # ì˜µì…˜ (ê°œë³„ + ì§‘ê³„)
    *option_cols,
    'option_score', 'option_rate', 'option_weighted',
    
    # ì§€ì—­
    'is_metro',
    
    # ìƒí˜¸ì‘ìš©
    'age_option_interaction', 'age_mileage_interaction', 'model_option_interaction'
]

# Train/Test ë‹¤ì‹œ ë¶„ë¦¬ (is_train ë§ˆì»¤ ì‚¬ìš©)
train_df = df[df['is_train'] == 1].copy()
test_df = df[df['is_train'] == 0].copy()

X_train = train_df[feature_cols]
y_train = train_df['Price_log']  # ë¡œê·¸ ë³€í™˜ëœ ê°€ê²©
X_test = test_df[feature_cols]
y_test = test_df['Price_log']

print(f"âœ“ Feature: {len(feature_cols)}ê°œ")
print(f"âœ“ Train: {len(X_train):,}í–‰")
print(f"âœ“ Test: {len(X_test):,}í–‰")

# ========== 7. ëª¨ë¸ í•™ìŠµ ==========
print("\nğŸ”¥ Step 7: XGBoost í•™ìŠµ (ê°•ë ¥í•œ ì •ê·œí™”)...")

model = xgb.XGBRegressor(
    n_estimators=800,
    learning_rate=0.02,
    max_depth=6,              # ê°ì†Œ (ê³¼ì í•© ë°©ì§€)
    min_child_weight=5,        # ì¦ê°€ (ê³¼ì í•© ë°©ì§€)
    subsample=0.7,            # ê°ì†Œ
    colsample_bytree=0.7,     # ê°ì†Œ
    colsample_bylevel=0.7,
    gamma=1.0,                # ì¦ê°€ (ê³¼ì í•© ë°©ì§€)
    reg_alpha=2.0,            # L1 ì •ê·œí™” ê°•í™”
    reg_lambda=5.0,           # L2 ì •ê·œí™” ê°•í™”
    random_state=42,
    n_jobs=-1,
    verbosity=0
)

model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=100
)

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")

# ========== 8. ëª¨ë¸ í‰ê°€ ==========
print("\n" + "="*80)
print("ğŸ“ˆ Step 8: ëª¨ë¸ í‰ê°€")
print("="*80)

# ë¡œê·¸ ê³µê°„ì—ì„œ í‰ê°€
y_train_pred_log = model.predict(X_train)
y_test_pred_log = model.predict(X_test)

# ì›ë˜ ê°€ê²©ìœ¼ë¡œ ë³€í™˜
y_train_pred = np.expm1(y_train_pred_log)
y_test_pred = np.expm1(y_test_pred_log)
y_train_true = np.expm1(y_train)
y_test_true = np.expm1(y_test)

# Train ì„±ëŠ¥
train_mae = mean_absolute_error(y_train_true, y_train_pred)
train_r2 = r2_score(y_train_true, y_train_pred)
train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))

print(f"\nğŸ”µ Train ì„±ëŠ¥:")
print(f"   MAE:  {train_mae:.2f}ë§Œì›")
print(f"   RMSE: {train_rmse:.2f}ë§Œì›")
print(f"   RÂ²:   {train_r2:.4f}")

# Test ì„±ëŠ¥
test_mae = mean_absolute_error(y_test_true, y_test_pred)
test_r2 = r2_score(y_test_true, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))

print(f"\nğŸŸ¢ Test ì„±ëŠ¥:")
print(f"   MAE:  {test_mae:.2f}ë§Œì›")
print(f"   RMSE: {test_rmse:.2f}ë§Œì›")
print(f"   RÂ²:   {test_r2:.4f}")

print(f"\nğŸ“Š ê³¼ì í•© ì²´í¬:")
print(f"   Train-Test RÂ² ì°¨ì´: {train_r2 - test_r2:.4f}")
if (train_r2 - test_r2) < 0.10:
    print(f"   âœ… ê³¼ì í•© ì—†ìŒ!")
else:
    print(f"   âš ï¸ ê³¼ì í•© ì¡´ì¬")

# Feature Importance
print(f"\nâ­ Feature Importance (ìƒìœ„ 20ê°œ):")
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance.head(20).iterrows():
    print(f"   {row['feature']:35s}: {row['importance']:.4f}")

# ========== 9. ëª¨ë¸ ì €ì¥ ==========
print("\n" + "="*80)
print("ğŸ’¾ Step 9: ëª¨ë¸ ì €ì¥")
print("="*80)

os.makedirs('models', exist_ok=True)

joblib.dump(model, 'models/domestic_ultimate.pkl')
joblib.dump(encoders, 'models/domestic_ultimate_encoders.pkl')
joblib.dump(feature_cols, 'models/domestic_ultimate_features.pkl')

metrics = {
    'train_mae': train_mae, 'train_r2': train_r2, 'train_rmse': train_rmse,
    'test_mae': test_mae, 'test_r2': test_r2, 'test_rmse': test_rmse,
    'n_samples': len(df), 'n_features': len(feature_cols),
    'overfitting_gap': train_r2 - test_r2,
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}
joblib.dump(metrics, 'models/domestic_ultimate_metrics.pkl')

print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

print("\n" + "="*80)
print("ğŸ‰ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("="*80)
print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"   Test RÂ²:  {test_r2:.4f}")
print(f"   Test MAE: {test_mae:.2f}ë§Œì›")
print(f"   Overfit Gap: {train_r2 - test_r2:.4f}")
print(f"   ë°ì´í„°: {len(df):,}ëŒ€")
print("="*80)
