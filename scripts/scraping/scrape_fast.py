"""
ì´ˆê³ ì† ì—”ì¹´ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ëŸ¬
- ë©€í‹°ìŠ¤ë ˆë”© (ë™ì‹œ 20ê°œ ìš”ì²­)
- Sleep ì‹œê°„ ìµœì†Œí™” (0.05~0.15ì´ˆ)
- í•µì‹¬ ë°ì´í„°ë§Œ ìˆ˜ì§‘
"""
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import json
import os
from datetime import datetime
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

class FastEncarScraper:
    def __init__(self, checkpoint_file='data/fast_checkpoint.json', 
                 output_file='data/fast_encar_data.csv'):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.checkpoint_file = checkpoint_file
        self.output_file = output_file
        self.lock = Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'processed_ids': [], 'collected_data': []}
    
    def save_checkpoint(self, checkpoint_data):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs('data', exist_ok=True)
        with self.lock:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False)
    
    def scrape_detail_fast(self, car_id):
        """ë‹¨ì¼ ì°¨ëŸ‰ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (í•µì‹¬ë§Œ)"""
        url = f"https://fem.encar.com/cars/detail/{car_id}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=5)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            detail_info = {'car_id': car_id}
            
            # í•µì‹¬ ì •ë³´ë§Œ ìˆ˜ì§‘ (ì†ë„ ìš°ì„ )
            # 1. ë¬´ì‚¬ê³ 
            detail_info['is_accident_free'] = 1 if soup.find(text=lambda t: t and 'ë¬´ì‚¬ê³ ' in str(t)) else 0
            
            # 2. ì„±ëŠ¥ì ê²€ ë“±ê¸‰
            if soup.find(text=lambda t: t and 'ìš°ìˆ˜' in str(t)):
                detail_info['inspection_grade'] = 'excellent'
            elif soup.find(text=lambda t: t and 'ì–‘í˜¸' in str(t)):
                detail_info['inspection_grade'] = 'good'
            else:
                detail_info['inspection_grade'] = 'normal'
            
            # 3. ì£¼ìš” ì˜µì…˜ 5ê°œë§Œ
            detail_info['has_sunroof'] = 1 if soup.find(text=lambda t: t and 'ì„ ë£¨í”„' in str(t)) else 0
            detail_info['has_navigation'] = 1 if soup.find(text=lambda t: t and 'ë‚´ë¹„ê²Œì´ì…˜' in str(t)) else 0
            detail_info['has_leather_seat'] = 1 if soup.find(text=lambda t: t and 'ê°€ì£½ì‹œíŠ¸' in str(t)) else 0
            detail_info['has_smart_key'] = 1 if soup.find(text=lambda t: t and 'ìŠ¤ë§ˆíŠ¸í‚¤' in str(t)) else 0
            detail_info['has_rear_camera'] = 1 if soup.find(text=lambda t: t and 'í›„ë°©ì¹´ë©”ë¼' in str(t)) else 0
            
            # ì´ˆë‹¨ì‹œê°„ ëŒ€ê¸° (IP ì°¨ë‹¨ ë°©ì§€)
            time.sleep(random.uniform(0.05, 0.15))
            
            return detail_info
            
        except Exception as e:
            return None
    
    def scrape_batch(self, car_ids, checkpoint_data):
        """ë°°ì¹˜ í¬ë¡¤ë§ (ë©€í‹°ìŠ¤ë ˆë”©)"""
        collected = []
        
        # ìµœëŒ€ 20ê°œ ë™ì‹œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_id = {executor.submit(self.scrape_detail_fast, car_id): car_id 
                           for car_id in car_ids}
            
            for future in as_completed(future_to_id):
                car_id = future_to_id[future]
                try:
                    result = future.result()
                    if result:
                        with self.lock:
                            collected.append(result)
                            checkpoint_data['processed_ids'].append(car_id)
                            print(f"âœ“ {car_id} ({len(checkpoint_data['processed_ids'])}ê°œ ì™„ë£Œ)", end='\r')
                except Exception as e:
                    print(f"âœ— {car_id} ì‹¤íŒ¨: {e}")
        
        return collected
    
    def scrape_all_fast(self, source_file='encar_raw_domestic.csv', batch_size=100):
        """ì „ì²´ ê³ ì† ìˆ˜ì§‘"""
        print("="*80)
        print("ğŸš€ ì´ˆê³ ì† ì—”ì¹´ í¬ë¡¤ëŸ¬ (ë©€í‹°ìŠ¤ë ˆë”©)")
        print("="*80)
        print(f"âœ“ ë™ì‹œ ìš”ì²­: 20ê°œ")
        print(f"âœ“ Sleep: 0.05~0.15ì´ˆ")
        print(f"âœ“ ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
        print("="*80)
        print()
        
        # ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        if not os.path.exists(source_file):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {source_file}")
            return
        
        df = pd.read_csv(source_file)
        
        # ID ì»¬ëŸ¼ ì°¾ê¸°
        id_column = None
        for col in ['Id', 'id', 'car_id', 'ID']:
            if col in df.columns:
                id_column = col
                break
        
        if not id_column:
            print("âŒ ID ì»¬ëŸ¼ ì—†ìŒ")
            return
        
        all_ids = df[id_column].dropna().astype(int).unique().tolist()
        print(f"ğŸ“Š ì´ {len(all_ids):,}ê°œ ID")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_data = self.load_checkpoint()
        processed = set(checkpoint_data['processed_ids'])
        remaining = [cid for cid in all_ids if cid not in processed]
        
        print(f"âœ“ ì´ë¯¸ ì™„ë£Œ: {len(processed):,}ê°œ")
        print(f"âœ“ ë‚¨ì€ ì‘ì—…: {len(remaining):,}ê°œ")
        print()
        
        if not remaining:
            print("âœ… ëª¨ë‘ ì™„ë£Œ!")
            return
        
        start_time = time.time()
        
        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(remaining), batch_size):
            batch = remaining[i:i+batch_size]
            batch_num = i // batch_size + 1
            
            print(f"\n{'='*80}")
            print(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
            print(f"{'='*80}")
            
            # ë©€í‹°ìŠ¤ë ˆë”© ìˆ˜ì§‘
            batch_start = time.time()
            collected = self.scrape_batch(batch, checkpoint_data)
            batch_time = time.time() - batch_start
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_data['collected_data'].extend(collected)
            self.save_checkpoint(checkpoint_data)
            
            # ì§„í–‰ ìƒí™©
            total_collected = len(checkpoint_data['processed_ids'])
            progress = total_collected / len(all_ids) * 100
            elapsed = time.time() - start_time
            speed = total_collected / elapsed if elapsed > 0 else 0
            eta = (len(all_ids) - total_collected) / speed if speed > 0 else 0
            
            print(f"\nğŸ“Š ì§„í–‰ ìƒí™©:")
            print(f"   âœ“ ì™„ë£Œ: {total_collected:,}/{len(all_ids):,} ({progress:.1f}%)")
            print(f"   âš¡ ì†ë„: {speed:.1f}ê°œ/ì´ˆ ({batch_time:.1f}ì´ˆ/{len(batch)}ê°œ)")
            print(f"   â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta/60:.1f}ë¶„")
            
            # CSV ì €ì¥ (500ê°œë§ˆë‹¤)
            if total_collected % 500 == 0:
                self.save_to_csv(checkpoint_data['collected_data'])
        
        # ìµœì¢… ì €ì¥
        print(f"\n{'='*80}")
        print("ğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")
        self.save_to_csv(checkpoint_data['collected_data'])
        
        total_time = time.time() - start_time
        print(f"âœ… ì™„ë£Œ! ì´ {len(checkpoint_data['processed_ids']):,}ê°œ")
        print(f"â±ï¸ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"âš¡ í‰ê·  ì†ë„: {len(checkpoint_data['processed_ids'])/total_time:.1f}ê°œ/ì´ˆ")
        print("="*80)
    
    def save_to_csv(self, data):
        """CSV ì €ì¥"""
        if data:
            df = pd.DataFrame(data)
            df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
            print(f"âœ“ CSV ì €ì¥: {len(df)}í–‰")


if __name__ == "__main__":
    # êµ­ì‚°ì°¨ ê³ ì† ìˆ˜ì§‘
    print("ğŸš— êµ­ì‚°ì°¨ ê³ ì† ìˆ˜ì§‘ ì‹œì‘...")
    scraper_dom = FastEncarScraper(
        checkpoint_file='data/fast_checkpoint_domestic.json',
        output_file='data/fast_domestic_details.csv'
    )
    scraper_dom.scrape_all_fast(source_file='encar_raw_domestic.csv', batch_size=200)
    
    print("\n\n")
    
    # ìˆ˜ì…ì°¨ ê³ ì† ìˆ˜ì§‘
    print("ğŸš™ ìˆ˜ì…ì°¨ ê³ ì† ìˆ˜ì§‘ ì‹œì‘...")
    scraper_imp = FastEncarScraper(
        checkpoint_file='data/fast_checkpoint_imported.json',
        output_file='data/fast_imported_details.csv'
    )
    scraper_imp.scrape_all_fast(source_file='encar_imported_data.csv', batch_size=200)
